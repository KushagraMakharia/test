{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Development\\RAG\\Langchain\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in GPT4AllEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "from langchain_core.prompts.base import BasePromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = (\"Huyen C. Designing Machine Learning Systems...2022.pdf\")\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4all_embd = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = await FAISS.afrom_documents(pages, gpt4all_embd)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "# compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor, base_retriever=retriever\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is MLOps?\"\n",
    "# docs = retriever.invoke(query)\n",
    "# pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT4All(model=\"E:\\GPT4ALL\\Phi-3-mini-4k-instruct.Q4_0.gguf\", n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system = \"\"\"You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "# LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "# LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "# LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "# LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
    "\n",
    "# Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# qa_no_context = prompt | model | StrOutputParser()\n",
    "\n",
    "# hyde_chain = RunnablePassthrough.assign(hypothetical_document=qa_no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "\n",
    "def maintain_history(session_id):\n",
    "    if session_id not in history:\n",
    "        history[session_id] = ChatMessageHistory()\n",
    "    return history[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_query = \"\"\" Given the query and the message history, create a new query which can be interpreted without the chat history.\\\n",
    "Do not add any extra information other than which is available in the chat history and existing query.\\\n",
    "The reultant query must not exceed 50 words.\n",
    "\"\"\"\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextual_query),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextual_retriever = create_history_aware_retriever(model, retriever, contextual_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Huyen C. Designing Machine Learning Systems...2022.pdf', 'page': 18}, page_content='T H E  R E L A T I O N S H I P  B E T W E E N  M L O P S  A N D  M L\\nS Y S T E M S  D E S I G N\\nOps in MLOps comes from DevOps, short for Developments and Operations. T o\\noperationalize something means to bring it into production, which includes deploying,\\nmonitoring, and maintaining it. MLOps is a set of tools and best practices for bringing\\nML into production.\\nML systems design takes a system approach to MLOps, which means that it considers\\nan ML system holistically to ensure that all the components and their stakeholders can\\nwork together to satisfy the specified objectives and requirements.\\nFigur e 1-1. Differ ent components of an ML system. “ML algorithms” is usually what people think of\\nwhen they say machine learning, but it’ s only a small part of the entir e system.\\nThere are many excellent books about various ML algorithms. This book\\ndoesn’ t cover any specific algorithms in detail but rather helps readers\\nunderstand the entire ML system as a whole. In other words, this book’ s\\ngoal is to provide you with a framework to develop a solution that best\\nworks for your problem, regardless of which algorithm you might end up\\nusing. Algorithms might become outdated quickly as new algorithms are'),\n",
       " Document(metadata={'source': 'Huyen C. Designing Machine Learning Systems...2022.pdf', 'page': 330}, page_content='Chapter 10. Infrastructure and\\nT ooling for MLOps\\nIn Chapters 4  to 6 , we discussed the logic for developing ML systems. In\\nChapters 7  to 9 , we discussed the considerations for deploying, monitoring,\\nand continually updating an ML system. Up until now , we’ve assumed that\\nML practitioners have access to all the tools and infrastructure they need to\\nimplement that logic and carry out these considerations. However , that\\nassumption is far from being true. Many data scientists have told me that\\nthey know the right things to do for their ML systems, but they can’ t do\\nthem because their infrastructure isn’ t set up in a way that enables them to\\ndo so.\\nML systems are complex. The more complex a system, the more it can\\nbenefit from good infrastructure. Infrastructure, when set up right, can help\\nautomate processes, reducing the need for specialized knowledge and\\nengineering time. This, in turn, can speed up the development and delivery\\nof ML applications, reduce the surface area for bugs, and enable new use\\ncases. When set up wrong, however , infrastructure is painful to use and\\nexpensive to replace. In this chapter , we’ll discuss how to set up\\ninfrastructure right for ML systems.\\nBefore we dive in, it’ s important to note that every company’ s infrastructure\\nneeds are dif ferent. The infrastructure required for you depends on the\\nnumber of applications you develop and how specialized the applications\\nare. At one end of the spectrum, you have companies that use ML for ad\\nhoc business analytics such as to project the number of new users they’ll\\nhave next year to present at their quarterly planning meeting. These\\ncompanies probably won’ t need to invest in any infrastructure—Jupyter\\nNotebooks, Python, and Pandas would be their best friends. If you have\\nonly one simple ML use case, such as an Android app for object detection'),\n",
       " Document(metadata={'source': 'Huyen C. Designing Machine Learning Systems...2022.pdf', 'page': 423}, page_content='ML models, model store , Model Store\\ndependency failure , Software System Failures\\ndeployment , Iterative Process , Model Deployment and Prediction Service\\nendpoints, exposing , Model Deployment and Prediction Service\\nfailure , Software System Failures\\nML models , Model Deployment\\nmyths\\nlimited models at once , Myth 1: Y ou Only Deploy One or T wo\\nML Models at a T ime - Myth 1: Y ou Only Deploy One or T wo ML\\nModels at a T ime\\nmodel updating , Myth 3: Y ou W on’ t Need to Update Y our Models\\nas Much\\nperformance , Myth 2: If W e Don’ t Do Anything, Model\\nPerformance Remains the Same\\nscale , Myth 4: Most ML Engineers Don’ t Need to W orry About\\nScale\\nseparation of responsibilities , Model Deployment and Prediction\\nService\\nshadow deployment , Shadow Deployment\\ndevelopment environment, infrastructure , Infrastructure and T ooling for\\nMLOps , Development Environment\\ncontainers , From Dev to Prod: Containers - From Dev to Prod:\\nContainers\\nsetup , Dev Environment Setup\\nIDE , IDE - IDE'),\n",
       " Document(metadata={'source': 'Huyen C. Designing Machine Learning Systems...2022.pdf', 'page': 333}, page_content='ML platform\\nThis provides tools to aid the development of ML applications such as\\nmodel stores, feature stores, and monitoring tools. Examples of tools in\\nthis category include SageMaker and MLflow .\\nDevelopment envir onment\\nThis is usually referred to as the dev environment; it is where code is\\nwritten and experiments are run. Code needs to be versioned and tested.\\nExperiments need to be tracked.\\nThese four dif ferent layers are shown in Figure 10-2 . Data and compute are\\nthe essential resources needed for any ML project, and thus the storage and\\ncompute layer  forms the infrastructural foundation for any company that\\nwants to apply ML. This layer is also the most abstract to a data scientist.\\nW e’ll discuss this layer first because these resources are the easiest to\\nexplain.\\nFigur e 10-2. Differ ent layers of infrastructur e for ML\\nThe dev environment is what data scientists have to interact with daily , and\\ntherefore, it is the least abstract to them. W e’ll discuss this category next,')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_retriever.invoke({\"input\": \"What is the need for MLOps?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_retrieval_chain() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m hyde_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mGiven the query, create a short hypothetical response for it under 100 words.\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      6\u001b[0m hyde_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m      7\u001b[0m     [\n\u001b[0;32m      8\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, hyde_system_prompt),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     ]\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m hyde_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_retrieval_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontextual_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyde_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: create_retrieval_chain() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# HyDE Retriver on contextual_retriever\n",
    "hyde_system_prompt = \"\"\" \n",
    "Given the query, create a short hypothetical response for it under 100 words.\n",
    "\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hyde_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hyde_retriever = create_retrieval_chain(model, contextual_retriever, hyde_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\" Given the query and its context, create a short and concise response. \\\n",
    " context: {context}\\\n",
    " \\n\n",
    " Assistant: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(contextual_retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    maintain_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Retriver + History Aware -> Mod Prompt -> Retriver -> Context -> LLM\n",
    "\n",
    "Retriver + History Aware -> Mod Prompt -> Hyde Retriver -> Retriver -> Context -> LLM\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_response = []\n",
    "async for events in conversational_rag_chain.astream_events(\n",
    "        {\"input\": \"What is the need for MLOps?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"4\"}\n",
    "    }, version=\"v1\"\n",
    "):\n",
    "    kind = events[\"event\"]\n",
    "    if kind == 'on_llm_stream':\n",
    "        data_chunk = events['data']['chunk']\n",
    "        # if len(data_chunk.strip()) > 0:\n",
    "        gen_response.append(data_chunk)\n",
    "        print(\"\".join(gen_response))\n",
    "    #{'event': 'on_llm_stream', 'name': 'GPT4All', 'run_id': '87927e82-97ce-4033-9cb4-886edd4b332c', 'tags': ['seq:step:3'], 'metadata': {'session_id': '1', 'ls_provider': 'gpt4all', 'ls_model_type': 'llm', 'ls_model_name': 'E:\\\\GPT4ALL\\\\Phi-3-mini-4k-instruct.Q4_0.gguf', 'ls_max_tokens': 200}, 'data': {'chunk': 'n'}, 'parent_ids': []}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_response = []\n",
    "async for events in conversational_rag_chain.astream_events(\n",
    "        {\"input\": \"What are some tools to implement it?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"4\"}\n",
    "    }, version=\"v1\"\n",
    "):\n",
    "    kind = events[\"event\"]\n",
    "    if kind == 'on_llm_stream':\n",
    "        data_chunk = events['data']['chunk']\n",
    "        # if len(data_chunk.strip()) > 0:\n",
    "        gen_response.append(data_chunk)\n",
    "        print(\"\".join(gen_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
